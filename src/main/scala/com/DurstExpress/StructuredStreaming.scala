package com.DurstExpress

import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql._
import org.apache.spark.sql.functions._

object StructuredStreaming {
  def main(args: Array[String]): Unit = {

    // Set Logging Level to Error Only
    Logger.getLogger("org").setLevel(Level.ERROR)

    //Start Spark Session
    val spark: SparkSession = SparkSession
      .builder()
      .master("local[*]")
      .appName("RealTimeUserAggregation")
      .getOrCreate()

    //Create a variable with a sample json line to infer its schema automatically
    val jsonString =
      """{"id":1,"first_name":"Barthel","last_name":"Kittel","email":"bkittel0@printfriendly.com",
        |"gender":"Male","ip_address":"130.187.82.195","date":"06/05/2018","country":"France"},"""".stripMargin

    //jsonDf.schema has the nested json structure we need
    import spark.implicits._
    val jsonDf = spark.read.json(Seq(jsonString).toDS) //

    //Begin to read from producer (This dataSet has all Kafka parameters and our data. Data is in Value Column)
    val kafkaDS = spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", "localhost:9092")
      .option("subscribe", "Users4")
      .option("startingOffsets", "earliest") // From starting
      .load()

    //Parse dataSet by getting Value Column and parse json object + add timestamp column that is generated by Kafka
    val parsedDS = kafkaDS.select($"timestamp",$"value" cast "string" as "json")
   .select($"timestamp", from_json($"json", jsonDf.schema) as "data").select($"data.*",$"timestamp")

    // Create a Temporary View to run your SparkSQL queries on the data
    parsedDS.createOrReplaceTempView("Customers")

    //Please activate the sink.awaitTermination and comment out the others to see the result.

    // Sample Spark SQL Querying
    /*spark
      .sql("SELECT * FROM Customers WHERE LIMIT 5")
      .writeStream
      .format("console")
      .start()
      .awaitTermination()*/

    // **************** AGGREGATION #1 ****************
    val mostCommonCountries = parsedDS
      .groupBy(
        window($"timestamp", "1 hours"),
        $"country")
      .count()
      .filter("`count` >= 10").sort(col("count").desc).limit(10)

    val mostCommonCountries_sink = mostCommonCountries
      .writeStream
      .format("console")
      .outputMode("complete")
      .option("truncate", "false")
      .start()

    //mostCommonCountries_sink.awaitTermination() //Activate the line to fetch the result onto the console
    // **************** AGGREGATION #1 ****************

    // **************** AGGREGATION #2 ****************
    val leastCommonCountries = parsedDS
      .groupBy(
        window($"timestamp", "1 hours"),
        $"country")
      .count()
      .sort(col("count")).limit(10)

    val leastCommonCountries_sink = mostCommonCountries
      .writeStream
      .format("console")
      .outputMode("complete")
      .option("truncate", "false")
      .start()

    //leastCommonCountries_sink.awaitTermination() //Activate the line to fetch the result onto the console
    // **************** AGGREGATION #2 ****************

    // **************** AGGREGATION #3 ****************
    val genderDistribution = parsedDS
      .groupBy(
        window($"timestamp", "1 hours"),
        $"gender")
      .count()
      .sort(col("count")).limit(10)

    val genderDistribution_sink = mostCommonCountries
      .writeStream
      .format("console")
      .outputMode("complete")
      .option("truncate", "false")
      .start()

    //genderDistribution.awaitTermination() //Activate the line to fetch the result onto the console

    // **************** AGGREGATION #3 ****************

  }
}
